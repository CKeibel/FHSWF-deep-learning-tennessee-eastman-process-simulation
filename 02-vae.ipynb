{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8875558-e752-48e1-ac01-84b17dba242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pyreadr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0347738-3a4d-4d27-a4c3-dbded07c1124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>faultNumber</th>\n",
       "      <th>simulationRun</th>\n",
       "      <th>sample</th>\n",
       "      <th>xmeas_1</th>\n",
       "      <th>xmeas_2</th>\n",
       "      <th>xmeas_3</th>\n",
       "      <th>xmeas_4</th>\n",
       "      <th>xmeas_5</th>\n",
       "      <th>xmeas_6</th>\n",
       "      <th>...</th>\n",
       "      <th>xmv_2</th>\n",
       "      <th>xmv_3</th>\n",
       "      <th>xmv_4</th>\n",
       "      <th>xmv_5</th>\n",
       "      <th>xmv_6</th>\n",
       "      <th>xmv_7</th>\n",
       "      <th>xmv_8</th>\n",
       "      <th>xmv_9</th>\n",
       "      <th>xmv_10</th>\n",
       "      <th>xmv_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25171</td>\n",
       "      <td>3672.4</td>\n",
       "      <td>4466.3</td>\n",
       "      <td>9.5122</td>\n",
       "      <td>27.057</td>\n",
       "      <td>42.473</td>\n",
       "      <td>...</td>\n",
       "      <td>54.494</td>\n",
       "      <td>24.527</td>\n",
       "      <td>59.710</td>\n",
       "      <td>22.357</td>\n",
       "      <td>40.149</td>\n",
       "      <td>40.074</td>\n",
       "      <td>47.955</td>\n",
       "      <td>47.300</td>\n",
       "      <td>42.100</td>\n",
       "      <td>15.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.25234</td>\n",
       "      <td>3642.2</td>\n",
       "      <td>4568.7</td>\n",
       "      <td>9.4145</td>\n",
       "      <td>26.999</td>\n",
       "      <td>42.586</td>\n",
       "      <td>...</td>\n",
       "      <td>53.269</td>\n",
       "      <td>24.465</td>\n",
       "      <td>60.466</td>\n",
       "      <td>22.413</td>\n",
       "      <td>39.956</td>\n",
       "      <td>36.651</td>\n",
       "      <td>45.038</td>\n",
       "      <td>47.502</td>\n",
       "      <td>40.553</td>\n",
       "      <td>16.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.24840</td>\n",
       "      <td>3643.1</td>\n",
       "      <td>4507.5</td>\n",
       "      <td>9.2901</td>\n",
       "      <td>26.927</td>\n",
       "      <td>42.278</td>\n",
       "      <td>...</td>\n",
       "      <td>54.000</td>\n",
       "      <td>24.860</td>\n",
       "      <td>60.642</td>\n",
       "      <td>22.199</td>\n",
       "      <td>40.074</td>\n",
       "      <td>41.868</td>\n",
       "      <td>44.553</td>\n",
       "      <td>47.479</td>\n",
       "      <td>41.341</td>\n",
       "      <td>20.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25153</td>\n",
       "      <td>3628.3</td>\n",
       "      <td>4519.3</td>\n",
       "      <td>9.3347</td>\n",
       "      <td>26.999</td>\n",
       "      <td>42.330</td>\n",
       "      <td>...</td>\n",
       "      <td>53.860</td>\n",
       "      <td>24.553</td>\n",
       "      <td>61.908</td>\n",
       "      <td>21.981</td>\n",
       "      <td>40.141</td>\n",
       "      <td>40.066</td>\n",
       "      <td>48.048</td>\n",
       "      <td>47.440</td>\n",
       "      <td>40.780</td>\n",
       "      <td>17.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.21763</td>\n",
       "      <td>3655.8</td>\n",
       "      <td>4571.0</td>\n",
       "      <td>9.3087</td>\n",
       "      <td>26.901</td>\n",
       "      <td>42.402</td>\n",
       "      <td>...</td>\n",
       "      <td>53.307</td>\n",
       "      <td>21.775</td>\n",
       "      <td>61.891</td>\n",
       "      <td>22.412</td>\n",
       "      <td>37.696</td>\n",
       "      <td>38.295</td>\n",
       "      <td>44.678</td>\n",
       "      <td>47.530</td>\n",
       "      <td>41.089</td>\n",
       "      <td>18.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15329995</th>\n",
       "      <td>4999995</td>\n",
       "      <td>20.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>496</td>\n",
       "      <td>0.23419</td>\n",
       "      <td>3655.3</td>\n",
       "      <td>4461.7</td>\n",
       "      <td>9.3448</td>\n",
       "      <td>27.008</td>\n",
       "      <td>42.481</td>\n",
       "      <td>...</td>\n",
       "      <td>53.670</td>\n",
       "      <td>23.350</td>\n",
       "      <td>61.061</td>\n",
       "      <td>20.719</td>\n",
       "      <td>40.999</td>\n",
       "      <td>38.653</td>\n",
       "      <td>47.386</td>\n",
       "      <td>47.528</td>\n",
       "      <td>40.212</td>\n",
       "      <td>17.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15329996</th>\n",
       "      <td>4999996</td>\n",
       "      <td>20.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>497</td>\n",
       "      <td>0.26704</td>\n",
       "      <td>3647.4</td>\n",
       "      <td>4540.2</td>\n",
       "      <td>9.3546</td>\n",
       "      <td>27.034</td>\n",
       "      <td>42.671</td>\n",
       "      <td>...</td>\n",
       "      <td>54.650</td>\n",
       "      <td>26.362</td>\n",
       "      <td>60.020</td>\n",
       "      <td>20.263</td>\n",
       "      <td>41.579</td>\n",
       "      <td>33.624</td>\n",
       "      <td>47.536</td>\n",
       "      <td>47.647</td>\n",
       "      <td>41.199</td>\n",
       "      <td>18.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15329997</th>\n",
       "      <td>4999997</td>\n",
       "      <td>20.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>498</td>\n",
       "      <td>0.26543</td>\n",
       "      <td>3630.3</td>\n",
       "      <td>4571.6</td>\n",
       "      <td>9.4089</td>\n",
       "      <td>27.129</td>\n",
       "      <td>42.470</td>\n",
       "      <td>...</td>\n",
       "      <td>54.274</td>\n",
       "      <td>26.521</td>\n",
       "      <td>59.824</td>\n",
       "      <td>20.189</td>\n",
       "      <td>41.505</td>\n",
       "      <td>40.967</td>\n",
       "      <td>52.437</td>\n",
       "      <td>47.802</td>\n",
       "      <td>41.302</td>\n",
       "      <td>23.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15329998</th>\n",
       "      <td>4999998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>499</td>\n",
       "      <td>0.27671</td>\n",
       "      <td>3655.7</td>\n",
       "      <td>4498.9</td>\n",
       "      <td>9.3781</td>\n",
       "      <td>27.353</td>\n",
       "      <td>42.281</td>\n",
       "      <td>...</td>\n",
       "      <td>53.506</td>\n",
       "      <td>26.781</td>\n",
       "      <td>62.818</td>\n",
       "      <td>20.453</td>\n",
       "      <td>40.208</td>\n",
       "      <td>40.957</td>\n",
       "      <td>47.628</td>\n",
       "      <td>48.086</td>\n",
       "      <td>40.510</td>\n",
       "      <td>15.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15329999</th>\n",
       "      <td>4999999</td>\n",
       "      <td>20.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.27421</td>\n",
       "      <td>3640.4</td>\n",
       "      <td>4474.4</td>\n",
       "      <td>9.3866</td>\n",
       "      <td>27.145</td>\n",
       "      <td>41.985</td>\n",
       "      <td>...</td>\n",
       "      <td>53.800</td>\n",
       "      <td>27.027</td>\n",
       "      <td>59.757</td>\n",
       "      <td>20.157</td>\n",
       "      <td>40.326</td>\n",
       "      <td>36.039</td>\n",
       "      <td>48.885</td>\n",
       "      <td>48.170</td>\n",
       "      <td>41.115</td>\n",
       "      <td>15.752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15330000 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index  faultNumber  simulationRun  sample  xmeas_1  xmeas_2  \\\n",
       "0               0          0.0            1.0       1  0.25171   3672.4   \n",
       "1               1          0.0            1.0       2  0.25234   3642.2   \n",
       "2               2          0.0            1.0       3  0.24840   3643.1   \n",
       "3               3          0.0            1.0       4  0.25153   3628.3   \n",
       "4               4          0.0            1.0       5  0.21763   3655.8   \n",
       "...           ...          ...            ...     ...      ...      ...   \n",
       "15329995  4999995         20.0          500.0     496  0.23419   3655.3   \n",
       "15329996  4999996         20.0          500.0     497  0.26704   3647.4   \n",
       "15329997  4999997         20.0          500.0     498  0.26543   3630.3   \n",
       "15329998  4999998         20.0          500.0     499  0.27671   3655.7   \n",
       "15329999  4999999         20.0          500.0     500  0.27421   3640.4   \n",
       "\n",
       "          xmeas_3  xmeas_4  xmeas_5  xmeas_6  ...   xmv_2   xmv_3   xmv_4  \\\n",
       "0          4466.3   9.5122   27.057   42.473  ...  54.494  24.527  59.710   \n",
       "1          4568.7   9.4145   26.999   42.586  ...  53.269  24.465  60.466   \n",
       "2          4507.5   9.2901   26.927   42.278  ...  54.000  24.860  60.642   \n",
       "3          4519.3   9.3347   26.999   42.330  ...  53.860  24.553  61.908   \n",
       "4          4571.0   9.3087   26.901   42.402  ...  53.307  21.775  61.891   \n",
       "...           ...      ...      ...      ...  ...     ...     ...     ...   \n",
       "15329995   4461.7   9.3448   27.008   42.481  ...  53.670  23.350  61.061   \n",
       "15329996   4540.2   9.3546   27.034   42.671  ...  54.650  26.362  60.020   \n",
       "15329997   4571.6   9.4089   27.129   42.470  ...  54.274  26.521  59.824   \n",
       "15329998   4498.9   9.3781   27.353   42.281  ...  53.506  26.781  62.818   \n",
       "15329999   4474.4   9.3866   27.145   41.985  ...  53.800  27.027  59.757   \n",
       "\n",
       "           xmv_5   xmv_6   xmv_7   xmv_8   xmv_9  xmv_10  xmv_11  \n",
       "0         22.357  40.149  40.074  47.955  47.300  42.100  15.345  \n",
       "1         22.413  39.956  36.651  45.038  47.502  40.553  16.063  \n",
       "2         22.199  40.074  41.868  44.553  47.479  41.341  20.452  \n",
       "3         21.981  40.141  40.066  48.048  47.440  40.780  17.123  \n",
       "4         22.412  37.696  38.295  44.678  47.530  41.089  18.681  \n",
       "...          ...     ...     ...     ...     ...     ...     ...  \n",
       "15329995  20.719  40.999  38.653  47.386  47.528  40.212  17.659  \n",
       "15329996  20.263  41.579  33.624  47.536  47.647  41.199  18.741  \n",
       "15329997  20.189  41.505  40.967  52.437  47.802  41.302  23.199  \n",
       "15329998  20.453  40.208  40.957  47.628  48.086  40.510  15.932  \n",
       "15329999  20.157  40.326  36.039  48.885  48.170  41.115  15.752  \n",
       "\n",
       "[15330000 rows x 56 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for path in glob.glob(\"data/*.RData\"):\n",
    "    _df = pyreadr.read_r(path)\n",
    "    k = list(_df.keys())[0]\n",
    "    _df =  _df[k]\n",
    "    df = pd.concat([df, _df])\n",
    "\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0acaca-9f75-432d-96e1-be13f1ae8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "df[\"faultNumber\"] = df[\"faultNumber\"].astype(int)\n",
    "df = df.drop([\"simulationRun\", \"sample\", \"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01a8a8e-c6be-4163-ab47-2501f81d464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 3, 9 and 15\n",
    "mask = ~df[\"faultNumber\"].isin([3, 9, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4c2db25-f3c7-4931-b47a-1ad7d46c3013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 16, 17, 18, 19,\n",
       "       20])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[mask]\n",
    "df[\"faultNumber\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730b095e-2205-49bc-a222-7206b53af6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13140000, 52), (13140000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features\n",
    "X = df.loc[:, df.columns != \"faultNumber\"].values\n",
    "# labels\n",
    "y = df[\"faultNumber\"].values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec06a15-0cbc-4501-93ec-b3868b473434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(X, y, lookback=5):\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    with tqdm(total=len(X)-lookback-1) as pbar:\n",
    "        for i in range(len(X)-lookback-1):\n",
    "            _x = X[i:i+lookback, :]\n",
    "            _y = y[i+lookback+1]\n",
    "            x_out.append(_x)\n",
    "            y_out.append(_y)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"Preparing numpy return. This could take some seconds.\")\n",
    "    return np.array(x_out), np.array(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4817c47e-769b-4f91-a525-12a81b22b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 13139994/13139994 [00:05<00:00, 2405361.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing numpy return. This could take some seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((13139994, 5, 52), (13139994,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_x, _y = create_samples(X, y)\n",
    "_x.shape, _y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4a550-912a-4a75-8a0c-1af5cb319cfe",
   "metadata": {},
   "source": [
    "### Scaler\n",
    "The recommended way (see 'Elements of Statistical Learning', chapter 'The Wrong and Right Way to Do Cross-validation') is to calculate the **mean** and the **standard deviation** of the values in the **training set** and then **apply them for standardizing both the training and testing sets**.\r\n",
    "\r\n",
    "The idea behind this is to preven**t data leaka**ge from the testing to the training set because the aim of model validation is to subject the testing data to the same conditions as the data used for the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a3aac-3b5f-45ca-a977-1bc9a1d58d74",
   "metadata": {},
   "source": [
    "[Link](https://datascience.stackexchange.com/questions/63717/how-to-use-standardization-standardscaler-for-train-and-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce063d4-f4c4-4606-a219-2aef1e8640b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(_x, _y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a2dc0-81e6-4f48-9f91-3a9daaa65341",
   "metadata": {},
   "source": [
    "# LSTM VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7d54e-2937-4659-9c1a-902976744fc9",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9a3254-b672-4554-beab-31a7690adca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        output, (hidden_state, cell_state) = self.lstm(X)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9055cb-99a1-428f-a674-86419529e039",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa8e0e8-7ab0-47b2-91d3-666d001dd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        # reconstruction\n",
    "        self.linear_recon = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        output, (hidden_state, cell_state) = self.lstm(X)\n",
    "        return self.linear_recon(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c87b16-432a-42a7-9af3-379924f0b369",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7a3f5ea-3585-4f07-819f-98bbfdfd9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, num_layers, device):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_size = latent_size\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            input_size=latent_size, # compressed vector size\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=input_size, # reconstruction of features at timestep\n",
    "            num_layers=num_layers\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.mean_linear = nn.Linear((self.hidden_size*self.num_layers), self.latent_size).to(self.device)\n",
    "        self.logvar_linear = nn.Linear((self.hidden_size*self.num_layers), self.latent_size).to(self.device)\n",
    "        #self.compressed_linear = nn.Linear(self.latent_size, self.hidden_size).to(self.device)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        # Gaussian (normal)\n",
    "        noise = torch.randn_like(std, device=self.device)\n",
    "        return mu + (noise * std)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len, features_dim = X.shape\n",
    "\n",
    "        # encode\n",
    "        # tensor of shape: 1*num_layers, batch_size, hidden_size\n",
    "        enc_hidden = self.encoder(X)\n",
    "        enc_hidden = enc_hidden.transpose(0, 1).contiguous().view(batch_size, -1) # (batch_size, hidden_size*num_layers)\n",
    "\n",
    "        # extract latent variable z (hidden to latent)\n",
    "        _mean = self.mean_linear(enc_hidden)\n",
    "        _logvar = self.logvar_linear(enc_hidden)\n",
    "        _z = self.reparametrize(_mean, _logvar) # Shape: batch_size, latent_size\n",
    "        _z = _z.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # hidden state\n",
    "        #_h = self.compressed_linear(_z) # batch_size, hidden_size\n",
    "        #_h = _h.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        pred = self.decoder(_z)\n",
    "\n",
    "        return pred, _mean, _logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "423bc8ef-0dd2-425f-bd40-962712437a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(pred, label, _mean, _logvar):\n",
    "    #reconstruction_loss = nn.BCELoss(pred, label)\n",
    "    reconstruction_loss = F.mse_loss(pred, label)\n",
    "    kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + _logvar - _mean**2 - _logvar.exp(), dim=1), dim=0\n",
    "        )\n",
    "    loss = reconstruction_loss + kld_loss #*kld_weight\n",
    "    return loss, reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1cb3932-605a-4cfe-bfee-bb72d10a1a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cb6dd49-1a2d-4a8f-864b-88e3ff32b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEP(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(TEP, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _x = self.x[idx]\n",
    "        _y = self.y[idx]\n",
    "        return _x, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea0cdb39-8e2d-4721-8883-3b2413c30046",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TEP(X_train, y_train)\n",
    "trainloader = DataLoader(train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec332b65-19b3-48a6-96c3-c0276605dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, samle_y = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39139ae4-6b18-4166-ba47-eab813942402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 52])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78adf156-ec52-451b-9d84-680ac5d77605",
   "metadata": {},
   "source": [
    "## Overfitting on one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50175ed1-cfb3-4c95-a0ab-0af639faac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(879209.2500, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "sample_x = sample_x.to(torch.float32).to(device)\n",
    "\n",
    "learning_rate = 0.002\n",
    "\n",
    "model = VAE(input_size=52, hidden_size=128, latent_size=16, num_layers=12, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for i in range(num_epochs):\n",
    "    pred, _mean, _logvar = model(sample_x)\n",
    "    loss, reconstruction_loss = vae_loss(pred, sample_x, _mean, _logvar)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #print(f\"Reconstruction Loss: {reconstruction_loss}\")\n",
    "\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "745a90df-f5ef-4608-a0d5-dceb44db44b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7958it [02:17, 58.03it/s, loss=9.9e+5] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m loss, reconstruction_loss \u001b[38;5;241m=\u001b[39m vae_loss(pred, x, _mean, _logvar)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m _r \u001b[38;5;241m=\u001b[39m reconstruction_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\Documents\\lernen\\modul_deep_learning\\.tep-deep-learning\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\lernen\\modul_deep_learning\\.tep-deep-learning\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# trainloader\n",
    "train_data = TEP(X_train, y_train)\n",
    "trainloader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "\n",
    "# model\n",
    "learning_rate = 0.0002\n",
    "model = VAE(input_size=52, hidden_size=128, latent_size=32, num_layers=8, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for i in range(num_epochs):\n",
    "    r_losses = []\n",
    "    loop = tqdm(enumerate(trainloader))\n",
    "    for _, (x, y) in loop:\n",
    "        x = x.to(torch.float32).to(device)\n",
    "        pred, _mean, _logvar = model(x)\n",
    "        loss, reconstruction_loss = vae_loss(pred, x, _mean, _logvar)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _r = reconstruction_loss.detach().item()\n",
    "        r_losses.append(_r)\n",
    "\n",
    "        loop.set_postfix(loss=_r)\n",
    "\n",
    "    print(f\"Reconstruction Loss: {np.mean(r_losses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tep-deep-learning",
   "language": "python",
   "name": ".tep-deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
